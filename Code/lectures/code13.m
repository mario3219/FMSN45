%
% Time series analysis
% Lund University
%
% Example code 13: Predicting the tobacco production (see also code 4)
%
% Reference: 
%   "An Introduction to Time Series Modeling", 4th ed, by Andreas Jakobsson
%   Studentlitteratur, 2021
%
clear; clc;
close all;
addpath('../functions', '../data')              % Add this line to update the path

% Plot the examined data. 
dataTobacco;

% Divide data into model and validation data set. At first limit the
% validation data as it seems the statistics change. Does it make a
% difference? How could one determine if there has been a trend change?
data = data(:);
N = 95;
%N = length(data);
modelLim  = 70;
modelData = data(1:modelLim);

figure(1)
time = 1871:1984;
plot( time, data, 'r' )
hold on
plot(time(1:modelLim), modelData,'b')
line( [time(modelLim) time(modelLim)], [-40 3000 ], 'Color','red','LineStyle',':' )
line( [time(N) time(N)], [-40 3000 ], 'Color','red','LineStyle',':' )
hold off
axis([time(1) time(end) 0 2500 ])
xlabel('Year')
ylabel('Production')
legend('Validation data', 'Model data', 'Location','SE')
title('US tobacco production')
noLags = 20;

% For debugging, try generating data that has similar characteristics.
% N = 10000; extraN = 100; time = 1:N;
% C  = [ 1 ];
% A = conv([1 -1],[ 1 0.37 ]);
% e  = randn( N+extraN, 1 );
% data = filter( C, A, e );     data = data(extraN+1:end) + 1000;
% modelData = data(1:N-1000);
% figure
% plot(data)
% noLags = 100;


%% Estimate unknown parameters with the model structure found in code 4.
% Differentiate to remove trend.
y = filter([1 -1], 1, modelData);   
y = y(2:end);

% Note that the found model is in the differentiated domain!
foundModel = estimateARMA( y, [1 1], [1], 'Differentiated data, \nabla y_t', noLags );


%% Lets form the k-step prediction.
% Predict future values. Some important things to note:
%   1) We start the prediction long before the validation data - here from
%      the beginning of the modeling data. This to save data; recall that
%      we need to omit ord(G) samples. 
%   2) The G polynomial will start with k zeros. It is (in general) not monic and is of order max( p-1, q-k ).
%   3) The F polynomial is monic of order k-1.
k = 3;                                              % Try other k, e.g., k=5.
foundModel.A = conv([1 -1], foundModel.A);          % Form the A polynomial taking the differentiation into account.
[F, G] = polydivision( foundModel.C, foundModel.A, k )   % Compute the G and F polynomials.
yhatk  = filter( G, foundModel.C, data );           % Form the predicted data.

% Looking at the the predictions, note that the inital k predicted values
% are zeros due to the zeros in the G polynomial. 
figure
plot(time, [data yhatk] )
hold on
line( [time(modelLim) time(modelLim)], [-40 max(data)*2 ], 'Color','red','LineStyle',':' )
line( [time(N) time(N)], [-40 3000 ], 'Color','red','LineStyle',':' )
hold off
axis([time(1) time(end) 0.8*min(data(10:end)) 1.2*max(data(10:end)) ])
legend('True data', sprintf('%i-step predition', k), 'Validation data', 'Location','NW')
title('US tobacco production')


%% How does the model work?
% We focus only on the validation data. Note that we now also remove the
% inital corrupted samples due to the filtering.
ehat = data - yhatk;
ehat = ehat(modelLim+1:N);

% Note that the prediction residual should only be white if k=1.
figure
acfEst = acf( ehat, noLags, 0.05, 1 );
title( sprintf('Prediction residual, %i-step prediction', k) )
fprintf('This is a %i-step prediction. Ideally, the residual should be an MA(%i) process.\n', k, k-1)
checkIfWhite( ehat );


%% What does the D'Agostino-Pearson's K2 test indicate?
% As the PACF should ring for an MA(k-1) process, we only check the ACF.
checkIfNormal( acfEst(k+1:end), 'ACF' );


%% Lets compare with a naive predictor
% If we assume no seasonality, the current sample might be a reasonable
% guess. This will cause a prediction that is delayed k steps.
testDataInd = modelLim+1:N;
[yNaive, var_naive] = naivePred(data, testDataInd, k );

figure
plot(time(testDataInd),[data(testDataInd) yhatk(testDataInd) yNaive])
title('It is reasonable to expect that we are better than the naive predictor...')
xlim([time(modelLim) time(N)])
legend('y','yhat','Naive')

fprintf('Prediction the signal %i-steps ahead.\n', k)
fprintf('  The variance of the prediction residual is       %5.2f\n', var(ehat))
fprintf('  The variance of the naive prediction residual is %5.2f\n', var_naive )
